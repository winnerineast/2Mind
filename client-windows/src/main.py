import time
import base64
import hashlib
from io import BytesIO
import argparse
import sys

import mss
import mss.tools
import cv2
import numpy as np
from PIL import Image
from openai import OpenAI
from colorama import init, Fore, Style

# === é…ç½®åŒºåŸŸ ===
VLLM_API_URL = "http://localhost:8000/v1"
API_KEY = "EMPTY"
MODEL_NAME = "Qwen/Qwen2-VL-7B-Instruct"

# è¡Œä¸ºé…ç½®
CHECK_INTERVAL = 0.5  # é‡‡æ ·é—´éš”
STABILITY_COUNT = 3  # ç¨³å®šæ¬¡æ•°é˜ˆå€¼
CAMERA_DIFF_THRESHOLD = 10.0  # æ‘„åƒå¤´åˆ¤å®šé™æ­¢çš„é˜ˆå€¼ (è°ƒå¤§ä¸€ç‚¹æ›´å®½æ¾)

init(autoreset=True)


def debug(msg, color=Fore.MAGENTA, end="\n"):
    """å¼ºåˆ¶åˆ·æ–°çš„è°ƒè¯•æ‰“å°"""
    print(f"{color}{msg}{Style.RESET_ALL}", end=end, flush=True)


class VisionSensor:
    def __init__(self, mode="screen", camera_index=0):
        self.mode = mode
        self.camera_index = camera_index
        self.cap = None

        if self.mode == "camera":
            debug(f"\n[Init] Start initializing Camera #{camera_index}...")

            # ç­–ç•¥ï¼šä¼˜å…ˆ DirectShow (ä¹Ÿå°±æ˜¯ probe.py æˆåŠŸçš„é‚£ä¸ª)
            # è¿™é‡Œçš„å…³é”®æ˜¯ä¸è®¾ç½®ä»»ä½•åˆ†è¾¨ç‡ï¼Œå®Œå…¨ä½¿ç”¨é»˜è®¤å€¼ï¼Œä»¥æ­¤ä¿è¯æœ€å¤§å…¼å®¹æ€§
            debug(f"[Init] Trying backend: cv2.CAP_DSHOW...")
            self.cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)

            if not self.cap.isOpened():
                debug(f"[Init] DSHOW failed! Trying cv2.CAP_MSMF...", Fore.YELLOW)
                self.cap = cv2.VideoCapture(camera_index, cv2.CAP_MSMF)

            if not self.cap.isOpened():
                debug(f"[Init] MSMF failed! Trying Auto...", Fore.YELLOW)
                self.cap = cv2.VideoCapture(camera_index)

            if not self.cap.isOpened():
                raise RuntimeError(f"âŒ Fatal: Could not open camera #{camera_index}")

            debug(f"[Init] Camera Opened! Reading warmup frame...", Fore.CYAN)
            ret, _ = self.cap.read()
            if not ret:
                debug(f"[Init] Warmup read failed!", Fore.RED)
            else:
                debug(f"[Init] Warmup read success.", Fore.GREEN)

    def capture(self):
        if self.mode == "screen":
            return self._capture_screen()
        elif self.mode == "camera":
            return self._capture_camera()

    def _capture_screen(self):
        with mss.mss() as sct:
            monitor = sct.monitors[1]
            sct_img = sct.grab(monitor)
            return Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")

    def _capture_camera(self):
        if not self.cap: return None

        # === å…³é”®è°ƒè¯•ç‚¹ ===
        # R = Requesting (æ­£åœ¨è¯·æ±‚ç¡¬ä»¶)
        # G = Got (ç¡¬ä»¶è¿”å›æ•°æ®)
        # å¦‚æœä½ åªçœ‹åˆ° R åé¢æ²¡ä¸œè¥¿ï¼Œå°±æ˜¯å¡æ­»åœ¨é©±åŠ¨å±‚äº†
        debug("R", Fore.BLACK, end="")

        ret, frame = self.cap.read()

        if not ret:
            debug("X", Fore.RED, end="")  # X = å¤±è´¥
            # å°è¯•é‡è¿
            # debug("\n[Error] Lost stream, reopening...", Fore.RED)
            # self.cap.open(self.camera_index)
            return None

        debug("G", Fore.BLACK, end="")

        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return Image.fromarray(rgb_frame)

    def release(self):
        if self.cap: self.cap.release()


class MindObserver:
    def __init__(self, sensor_mode="screen", camera_index=0):
        self.client = OpenAI(base_url=VLLM_API_URL, api_key=API_KEY)
        self.sensor = VisionSensor(mode=sensor_mode, camera_index=camera_index)
        self.last_frame_array = None
        self.last_hash = None

    def compress_image(self, image):
        # ç¼©å°å›¾ç‰‡ä»¥åŠ å¿«ä¼ è¾“
        image.thumbnail((1024, 1024))
        buffered = BytesIO()
        image.save(buffered, format="JPEG", quality=60)
        return base64.b64encode(buffered.getvalue()).decode('utf-8')

    def is_stable(self, current_img):
        if self.sensor.mode == "screen":
            small = current_img.resize((64, 64), Image.Resampling.NEAREST)
            current_hash = hashlib.md5(small.tobytes()).hexdigest()
            is_same = (current_hash == self.last_hash)
            self.last_hash = current_hash
            if not is_same: debug(".", Fore.CYAN, end="")
            return is_same
        else:
            # æ‘„åƒå¤´æ¨¡å¼ï¼šè®¡ç®—åƒç´ å·®
            current_array = np.array(current_img.resize((256, 256)))
            if self.last_frame_array is None:
                self.last_frame_array = current_array
                return False

            diff = cv2.absdiff(current_array, self.last_frame_array)
            mean_diff = np.mean(diff)
            self.last_frame_array = current_array

            # æ‰“å°å®æ—¶å·®å¼‚å€¼
            color = Fore.GREEN if mean_diff < CAMERA_DIFF_THRESHOLD else Fore.YELLOW
            debug(f"[{mean_diff:.1f}]", color, end="")

            return mean_diff < CAMERA_DIFF_THRESHOLD

    def ask_brain(self, b64_img):
        debug("\nğŸ§  Thinking... ", Fore.YELLOW)
        try:
            prompt = "åˆ†æè¿™ä¸ªç”»é¢ã€‚"
            if self.sensor.mode == "camera":
                prompt = "è¿™æ˜¯æ‘„åƒå¤´å®æ—¶ç”»é¢ã€‚ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿç®€çŸ­æè¿°ã€‚"

            response = self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=[
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}},
                    ]}
                ],
                max_tokens=100,
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error: {e}"

    def run(self):
        print(f"{Fore.CYAN}>>> Observer Started [{self.sensor.mode.upper()}]")
        stable_counter = 0

        try:
            while True:
                # === Windows OpenCV å¿…é¡»åŠ è¿™å¥ï¼Œå¦åˆ™ DirectShow ä¼šå¡æ­» ===
                if self.sensor.mode == "camera":
                    cv2.waitKey(1)

                start = time.time()
                img = self.sensor.capture()

                if img is None:
                    time.sleep(0.1)
                    continue

                # æ£€æµ‹é™æ­¢
                if self.is_stable(img):
                    stable_counter += 1
                else:
                    stable_counter = 0

                # è§¦å‘
                if stable_counter == STABILITY_COUNT:
                    print(f"\n{Fore.GREEN}[!] Stable. Analyzing...")
                    result = self.ask_brain(self.compress_image(img))
                    print(f"\n{Fore.WHITE}{Style.BRIGHT}{result}\n{'-' * 20}")
                    stable_counter += 1

                elapsed = time.time() - start
                time.sleep(max(0, CHECK_INTERVAL - elapsed))

        except KeyboardInterrupt:
            print(f"\n{Fore.RED}Stopped.")
            self.sensor.release()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="screen", choices=["screen", "camera"])
    parser.add_argument("--cam-index", type=int, default=0)
    args = parser.parse_args()

    try:
        MindObserver(sensor_mode=args.mode, camera_index=args.cam_index).run()
    except Exception as e:
        print(f"\n{Fore.RED}Fatal Error: {e}")