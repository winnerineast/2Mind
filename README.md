# 2Mind
This is OS-Copilot and/or Context-Aware Assistant built for ourselves by our own PC/Laptops (mobile phone coming soon...)

## Github Project Structure

#### 2mind/
##### â”œâ”€â”€ .github/                # GitHub Actions 
##### â”œâ”€â”€ assets/                 # architecture diagram, Demo pictures used by readme.md
##### â”œâ”€â”€ client-windows/         # Windows client source codes
##### â”‚   â”œâ”€â”€ src/
##### â”‚   â”‚   â”œâ”€â”€ observer.py     # capture screen
##### â”‚   â”‚   â”œâ”€â”€ overlay.py      # UI display (PyQt/Tkinter)
##### â”‚   â”‚   â””â”€â”€ utils.py        # Picture utilities
##### â”‚   â”œâ”€â”€ requirements.txt    # mss, openai, pillow, etc.
##### â”‚   â””â”€â”€ config.yaml         # configuration for VLM and applications.
##### â”œâ”€â”€ server-vllm/            # Linux/WSL configuration
##### â”‚   â”œâ”€â”€ start_server.sh     # vLLM script
##### â”‚   â”œâ”€â”€ models/             # (optional) additional adapter
##### â”‚   â””â”€â”€ requirements.txt    # vllm, flash-attn
##### â”œâ”€â”€ docs/                   # design documents
##### â”œâ”€â”€ .gitignore              # 
##### â”œâ”€â”€ README.md               # Entry of this project
##### â””â”€â”€ LICENSE                 # MIT License

## ðŸ— Architecture

```mermaid
graph LR
    subgraph "Windows 11 Host"
        A["ðŸ–¥ï¸ Screen Capture"]
        B["ðŸ Client Logic (Python)"]
        E["ðŸŽ¨ Overlay UI"]
    end

    subgraph "WSL2 Subsystem"
        C["ðŸ§  vLLM Server (Qwen2)"]
    end

    A -- "1. Grab Screen" --> B
    B -- "2. Send Base64" --> C
    C -- "3. JSON Result" --> E
    
    E -.-> A
```

## How to Run
### Step 1 - KYE (Know Your Environment)
Here is my testing environment:
* Windows 11 Pro (MEM 64GB/GPU 24GB)
* Pycharm 
* Minconda Python 3.13
* WSL2 (Ubuntu 22.04)
* CUDA 12.x